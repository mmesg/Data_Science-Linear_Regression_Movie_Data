{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62aed769",
   "metadata": {
    "id": "62aed769"
   },
   "source": [
    "### 1. Introduction: Project Overview\n",
    "\n",
    "This project centers around the comprehensive analysis of three distinct datasets: movie Data, critic reviews, and user reviews. The scope of the project encompasses various aspects, including:\n",
    "\n",
    "- Data Cleaning: Ensuring data integrity and quality for accurate analysis.\n",
    "- Data Analysis: Identifying and evaluating the best movies based on various metrics.\n",
    "- Data Visualization: Creating insightful visual representations of the data to facilitate understanding.\n",
    "- Linear Regression: Applying statistical methods to predict audience scores based on relevant features.\n",
    "- Natural Language Processing (NLP): Analyzing textual data from reviews to extract meaningful insights.\n",
    "\n",
    "Below are the summarized details of the three datasets presented in their respective dictionaries:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6614996",
   "metadata": {
    "id": "c6614996"
   },
   "source": [
    "#### Data Dictionary for `Movies`\n",
    "\n",
    "| Column Name               | Data Type | Description                                                                                           |\n",
    "| ------------------------- | --------- | ----------------------------------------------------------------------------------------------------- |\n",
    "| `movieId`                 | object    | Unique identifier for each movie.                                                                     |\n",
    "| `movieYear`               | int64     | The year the movie was released.                                                                      |\n",
    "| `movieURL`                | object    | URL to the movie's page on Rotten Tomatoes.                                                           |\n",
    "| `movieTitle`              | object    | The title of the movie.                                                                               |\n",
    "| `critic_score`            | float64   | Average score given by critics for the movie.                                                         |\n",
    "| `critic_sentiment`        | object    | Sentiment of the critic's reviews (e.g., positive, negative, neutral).                                 |\n",
    "| `audience_score`          | float64   | Average score given by audience members for the movie.                                                |\n",
    "| `audience_sentiment`      | object    | Sentiment of audience reviews (e.g., positive, negative, neutral).                                     |\n",
    "| `release_date_theaters`   | object    | The release date of the movie in theaters.                                                            |\n",
    "| `release_date_streaming`  | object    | The release date of the movie on streaming platforms.                                                 |\n",
    "| `rating`                  | object    | The movie's rating (e.g., PG, PG-13, R).                                                              |\n",
    "| `original_language`       | object    | The original language of the movie (e.g., English, French).                                           |\n",
    "| `runtime`                 | object    | The runtime of the movie in minutes (may be stored as text in some cases, e.g., \"120 minutes\").       |\n",
    "\n",
    "<br/>\n",
    "\n",
    "#### Data Dictionary for `Critic Reviews`\n",
    "\n",
    "| Column Name         | Data Type | Description                                                                                           |\n",
    "| ------------------- | --------- | ----------------------------------------------------------------------------------------------------- |\n",
    "| `reviewId`          | int64     | Unique identifier for each critic review.                                                             |\n",
    "| `movieId`           | object    | Unique identifier for the movie that the review corresponds to.                                        |\n",
    "| `creationDate`      | object    | The date when the review was created or published.                                                     |\n",
    "| `criticName`        | object    | The name of the critic who wrote the review.                                                           |\n",
    "| `criticPageUrl`     | object    | URL to the critic's page on Rotten Tomatoes or the publication website.                                |\n",
    "| `reviewState`       | object    | The state of the review (e.g., published, draft).                                                      |\n",
    "| `isFresh`           | bool      | A boolean indicator whether the review is categorized as \"Fresh\" (positive review).                    |\n",
    "| `isRotten`          | bool      | A boolean indicator whether the review is categorized as \"Rotten\" (negative review).                   |\n",
    "| `isRtUrl`           | object    | URL to the Rotten Tomatoes page for this review (if available).                                        |\n",
    "| `isTopCritic`       | bool      | A boolean indicator whether the review was written by a \"Top Critic\" on Rotten Tomatoes.               |\n",
    "| `publicationUrl`    | object    | URL to the publication's homepage or the article containing the review.                                |\n",
    "| `publicationName`   | object    | The name of the publication where the review was published.                                            |\n",
    "| `reviewUrl`         | object    | Direct URL to the individual review page on the publication's website.                                 |\n",
    "| `quote`             | object    | A brief excerpt or quote from the review.                                                              |\n",
    "| `scoreSentiment`    | object    | The sentiment of the review (e.g., positive, negative).                                                |\n",
    "| `originalScore`     | object    | The original score given by the critic, if available.                                |\n",
    "\n",
    "<br/>\n",
    "\n",
    "#### Data Dictionary for `User Reviews`\n",
    "\n",
    "| Column Name        | Data Type | Description                                                                                           |\n",
    "| ------------------ | --------- | ----------------------------------------------------------------------------------------------------- |\n",
    "| `movieId`          | object    | Unique identifier for the movie that the user review corresponds to.                                   |\n",
    "| `rating`           | float64   | The rating given by the user, typically a score between 0 and 10 (or similar scale).                   |\n",
    "| `quote`            | object    | A brief excerpt or comment from the user's review.                                                     |\n",
    "| `reviewId`         | object    | Unique identifier for each user review.                                                                |\n",
    "| `isVerified`       | bool      | A boolean indicating whether the user who wrote the review is a verified user.                         |\n",
    "| `isSuperReviewer`  | bool      | A boolean indicating whether the user is marked as a \"Super Reviewer\" (frequent and/or trusted user).  |\n",
    "| `hasSpoilers`      | bool      | A boolean indicating whether the review contains spoilers.                                             |\n",
    "| `hasProfanity`     | bool      | A boolean indicating whether the review contains profanity.                                            |\n",
    "| `score`            | float64   | The score given by the user (if different from `rating`).                                              |\n",
    "| `creationDate`     | object    | The date when the user review was created or published.                                                |\n",
    "| `userDisplayName`  | object    | The display name of the user who wrote the review.                                                     |\n",
    "| `userRealm`        | object    | The region or realm associated with the user (could be location-based information).                    |\n",
    "| `userId`           | object    | Unique identifier for the user who wrote the review.                                                   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24c07b4",
   "metadata": {
    "id": "c24c07b4"
   },
   "source": [
    "### 2. Importing Libraries and Loading Data\n",
    "To start our analysis, we import the necessary libraries for data manipulation, visualization, and natural language processing (NLP). Libraries such as Pandas, Matplotlib, Seaborn, NLTK, and Scikit-learn provide powerful tools for handling our datasets.\n",
    "\n",
    "We then load three CSV files containing movie details, critic reviews, and user reviews into separate DataFrames. This is the foundation for our analysis and will allow us to combine different perspectives on movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3d01c10",
   "metadata": {
    "executionInfo": {
     "elapsed": 25587,
     "status": "ok",
     "timestamp": 1728831629615,
     "user": {
      "displayName": "M.M.",
      "userId": "16283090766959631234"
     },
     "user_tz": -120
    },
    "id": "e3d01c10"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gdown\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from textblob import TextBlob\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import time\n",
    "import ast\n",
    "from nltk import pos_tag\n",
    "from collections import Counter\n",
    "import gc\n",
    "from nltk.tokenize import word_tokenize\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "import re\n",
    "import spacy\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f27fba64-5b85-4b16-b3dd-ec515764006c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 703
    },
    "executionInfo": {
     "elapsed": 44462,
     "status": "error",
     "timestamp": 1728831674076,
     "user": {
      "displayName": "M.M.",
      "userId": "16283090766959631234"
     },
     "user_tz": -120
    },
    "id": "H5bYoap52CK2",
    "outputId": "d2f276a7-8c58-4eb6-9a04-b555156dd23f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets loaded successfully in chunks.\n"
     ]
    }
   ],
   "source": [
    "# Load in smaller chunks instead of all at once\n",
    "def download_and_load_csv_in_chunks(file_id, filename, chunk_size=10000, low_memory=True, dtype=None):\n",
    "    url = f'https://drive.google.com/uc?export=download&id={file_id}'\n",
    "    gdown.download(url, filename, quiet=True)\n",
    "\n",
    "    # Use chunksize to load in small portions\n",
    "    df_chunks = pd.read_csv(filename, low_memory=low_memory, dtype=dtype, chunksize=chunk_size)\n",
    "\n",
    "    # Combine chunks to process them incrementally\n",
    "    return pd.concat(df_chunks, ignore_index=True)\n",
    "\n",
    "# Google Drive file IDs\n",
    "file_ids = {\n",
    "    'movies': '19lYJrl5eTUCyBa0rieheC4i5sIIOlotz',\n",
    "    'critic_reviews': '10XvJsU0vT8KK9-krIutzTMFN1NCGrXVD',\n",
    "    'user_reviews': '1nFDc_qCWm0AxJfGy653LMycMvUt7K-B3'\n",
    "}\n",
    "\n",
    "# Now load using the chunked approach\n",
    "movies = download_and_load_csv_in_chunks(file_ids['movies'], 'movies.csv', chunk_size=10000, low_memory=False)\n",
    "critic_reviews = download_and_load_csv_in_chunks(file_ids['critic_reviews'], 'critic_reviews.csv',chunk_size=10000, low_memory=False, dtype={'isRtUrl': 'boolean'})\n",
    "user_reviews = download_and_load_csv_in_chunks(file_ids['user_reviews'], 'sampled_user_reviews.csv',chunk_size=10000, low_memory=False)\n",
    "print(\"Datasets loaded successfully in chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c91026",
   "metadata": {
    "id": "84c91026"
   },
   "source": [
    "## 3. Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57a958d",
   "metadata": {
    "id": "d57a958d"
   },
   "source": [
    "### 3.1. Data Cleaning Functions\n",
    "\n",
    "**\"Camel Case to Snake Case Conversion\" Function**\n",
    "\n",
    "To ensure consistency, we convert column names from camelCase to snake_case using a function called camel_to_snake_manual(). This helps standardize column names for easier analysis and reference.\n",
    "\n",
    "**\"Standardizing Scores\" Function**\n",
    "\n",
    "The standardize_score() function is used to convert different score formats into a uniform scale (from 0 to 10). This allows us to compare scores across critics and users fairly.\n",
    "\n",
    "**\"Converting Runtime\" Function**\n",
    "\n",
    "The convert_runtime_to_minutes() function converts runtime values from hours and minutes format into total minutes. This ensures that runtime information is in a consistent and comparable numerical format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d28edf24",
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "aborted",
     "timestamp": 1728831674077,
     "user": {
      "displayName": "M.M.",
      "userId": "16283090766959631234"
     },
     "user_tz": -120
    },
    "id": "d28edf24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning functions defined.\n"
     ]
    }
   ],
   "source": [
    "def camel_to_snake_manual(name):\n",
    "    result = [name[0].lower()]\n",
    "    for char in name[1:]:\n",
    "        if char.isupper():\n",
    "            result.append('_')\n",
    "            result.append(char.lower())\n",
    "        else:\n",
    "            result.append(char)\n",
    "    return ''.join(result)\n",
    "\n",
    "def standardize_score(score):\n",
    "    try:\n",
    "        if '/' in score:\n",
    "            num, denom = map(float, score.split('/'))\n",
    "            return round((num / denom) * 10, 1)\n",
    "        else:\n",
    "            return float(score)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def convert_runtime_to_minutes(runtime):\n",
    "    if isinstance(runtime, str):\n",
    "        hours, minutes = 0, 0\n",
    "        if 'h' in runtime:\n",
    "            parts = runtime.split('h')\n",
    "            hours = int(parts[0].strip()) * 60\n",
    "            if 'm' in parts[1]:\n",
    "                minutes = int(parts[1].strip('m').strip())\n",
    "        elif 'm' in runtime:\n",
    "            minutes = int(runtime.strip('m').strip())\n",
    "        return hours + minutes\n",
    "    return None\n",
    "\n",
    "print(\"Cleaning functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47f925c",
   "metadata": {
    "id": "f47f925c"
   },
   "source": [
    "### 3.2. Cleaning Movie Data\n",
    "The clean_movies() function cleans the movie dataset by:\n",
    "\n",
    "- Dropping unnecessary columns (e.g., URLs).\n",
    "- Converting column names to snake_case.\n",
    "- Handling missing values for scores, sentiments, and runtime.\n",
    "- Extracting the release year from different date formats.\n",
    "- Standardizing the language categories for consistency.\n",
    "\n",
    "The clean_movies() function encompasses among other thing the three functions previously built: \"camel_to_snake_manual\", \"standardize_score\" and \"convert_runtime_to_minutes\".\n",
    "\n",
    "The goal is to create a clean and usable dataset ready for merging and further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca27e711-3f52-432c-9413-905e968e612c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movies data cleaned.\n"
     ]
    }
   ],
   "source": [
    "def clean_movies(df):\n",
    "    df_cleaned = df.copy()\n",
    "    df_cleaned = df_cleaned.drop(columns='movieURL')\n",
    "    df_cleaned.columns = [camel_to_snake_manual(col) for col in df_cleaned.columns]\n",
    "\n",
    "    # Fill missing scores and sentiments\n",
    "    df_cleaned['critic_score'] = df_cleaned['critic_score'].fillna(df_cleaned['critic_score'].mean())\n",
    "    df_cleaned['audience_score'] = df_cleaned['audience_score'].fillna(df_cleaned['audience_score'].mean())\n",
    "    df_cleaned['critic_sentiment'] = df_cleaned['critic_sentiment'].fillna('unknown')\n",
    "    df_cleaned['audience_sentiment'] = df_cleaned['audience_sentiment'].fillna('unknown')\n",
    "    df_cleaned['rating'] = df_cleaned['rating'].fillna('unknown')\n",
    "\n",
    "\n",
    "    # Extract release years\n",
    "    def extract_year(date_string):\n",
    "        try:\n",
    "            return pd.to_datetime(date_string, format='%Y-%m-%d', errors='coerce').year\n",
    "        except:\n",
    "            try:\n",
    "                return pd.to_datetime(date_string, format='%B %d, %Y', errors='coerce').year\n",
    "            except:\n",
    "                return pd.NaT\n",
    "\n",
    "    df_cleaned['release_year_theaters'] = df_cleaned['release_date_theaters'].apply(extract_year)\n",
    "    df_cleaned['release_year_streaming'] = df_cleaned['release_date_streaming'].apply(extract_year)\n",
    "    df_cleaned = df_cleaned.drop(columns=['release_date_theaters', 'release_date_streaming'])\n",
    "\n",
    "    # Round scores\n",
    "    df_cleaned['critic_score'] = df_cleaned['critic_score'].round(1)\n",
    "    df_cleaned['audience_score'] = df_cleaned['audience_score'].round(1)\n",
    "\n",
    "    # Standardize language\n",
    "    language_mapping = {\n",
    "        'English (United Kingdom)': 'English',\n",
    "        'English (Australia)': 'English',\n",
    "        'British English': 'English',\n",
    "        'Australian English': 'English',\n",
    "        'Portuguese (Brazil)': 'Portuguese',\n",
    "        'Brazilian Portuguese': 'Portuguese',\n",
    "        'French (France)': 'French',\n",
    "        'French (Canada)': 'French',\n",
    "        'Canadian French': 'French',\n",
    "        'Unknown language': 'Unknown'\n",
    "    }\n",
    "    df_cleaned['original_language'] = df_cleaned['original_language'].replace(language_mapping)\n",
    "    df_cleaned['original_language'].fillna('Unknown')\n",
    "\n",
    "    # Convert runtime to minutes\n",
    "    df_cleaned['runtime_in_minutes'] = df_cleaned['runtime'].apply(convert_runtime_to_minutes)\n",
    "    df_cleaned = df_cleaned.drop(columns='runtime')\n",
    "\n",
    "    return df_cleaned\n",
    "\n",
    "movies_cleaned = clean_movies(movies)\n",
    "print(\"Movies data cleaned.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zBauoJPZCp_0",
   "metadata": {
    "id": "zBauoJPZCp_0"
   },
   "source": [
    "### 3.3. Cleaning Critic Reviews\n",
    "The clean_critic_reviews() function:\n",
    "\n",
    "- Drops irrelevant columns.\n",
    "- Converts the creation date to a year format.\n",
    "- Handles missing values and filters out placeholder quotes that are not meaningful.\n",
    "- Converts critic scores to a standardized range of 1 to 10.\n",
    "\n",
    "This helps in getting more meaningful data about critics' opinions and ensuring that only relevant quotes and reviews are retained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "debe7ed3-d6f5-4fa4-a802-da7deb9469c1",
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "aborted",
     "timestamp": 1728831674077,
     "user": {
      "displayName": "M.M.",
      "userId": "16283090766959631234"
     },
     "user_tz": -120
    },
    "id": "p04FxCF9Ci30"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'critic_reviews' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11408\\3690119443.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdf_cleaned\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mcritic_reviews_cleaned\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclean_critic_reviews\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcritic_reviews\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Critic reviews cleaned.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'critic_reviews' is not defined"
     ]
    }
   ],
   "source": [
    "def clean_critic_reviews(df):\n",
    "    df_cleaned = df.copy()\n",
    "    df_cleaned.columns = [camel_to_snake_manual(col) for col in df_cleaned.columns]\n",
    "\n",
    "    columns_to_remove = ['critic_page_url', 'review_state', 'is_rotten', 'is_rt_url', 'publication_url', 'review_url']\n",
    "    df_cleaned = df_cleaned.drop(columns=columns_to_remove)\n",
    "\n",
    "    df_cleaned['creation_date'] = dd.to_datetime(df_cleaned['creation_date'], errors='coerce')\n",
    "    df_cleaned['creation_year'] = df_cleaned['creation_date'].dt.year\n",
    "    df_cleaned = df_cleaned.drop(columns=['creation_date'])\n",
    "\n",
    "    df_cleaned['critic_name'] = df_cleaned['critic_name'].fillna('Unknown Critic')\n",
    "    df_cleaned = df_cleaned.dropna(subset=['quote'])\n",
    "    df_cleaned = df_cleaned.drop_duplicates(subset=['review_id'], keep='first')\n",
    "\n",
    "    placeholders = [\"full review at Movies for the Masses\", \"full review in Greek\", \".\",\n",
    "                    \"click for full review\", \"Click to read review\", \"click to read full review\",\n",
    "                    \"See website for more details.\", \"click to read the full review\", \"(No quote available.)\"]\n",
    "    df_cleaned = df_cleaned[~df_cleaned['quote'].isin(placeholders)]\n",
    "    df_cleaned = df_cleaned[df_cleaned['quote'].str.len() > 5]\n",
    "\n",
    "    df_cleaned['standardized_score'] = df_cleaned['original_score'].apply(standardize_score)\n",
    "    mean_score = df_cleaned['standardized_score'].mean()\n",
    "    df_cleaned['standardized_score'] = df_cleaned['standardized_score'].fillna(round(mean_score))\n",
    "    df_cleaned['standardized_score'] = df_cleaned['standardized_score'].apply(lambda x: min(max(round(x), 1), 10))\n",
    "\n",
    "    df_cleaned = df_cleaned.drop(columns=['original_score'])\n",
    "\n",
    "    return df_cleaned\n",
    "\n",
    "critic_reviews_cleaned = clean_critic_reviews(critic_reviews)\n",
    "print(\"Critic reviews cleaned.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iLD9n0H6C5Sb",
   "metadata": {
    "id": "iLD9n0H6C5Sb"
   },
   "source": [
    "### 3.4. Cleaning User Reviews\n",
    "The clean_user_reviews() function cleans the user reviews by removing irrelevant columns, standardizing scores, and converting dates to a consistent year format. This is necessary for combining user feedback effectively with critic reviews and movie details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14515ba-649a-444c-8a02-cc1fe87b8302",
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "aborted",
     "timestamp": 1728831674077,
     "user": {
      "displayName": "M.M.",
      "userId": "16283090766959631234"
     },
     "user_tz": -120
    },
    "id": "r6mTqIbnC5gt"
   },
   "outputs": [],
   "source": [
    "def clean_user_reviews(df):\n",
    "    df_cleaned = df.copy()\n",
    "    df_cleaned = df_cleaned.drop(['reviewId', 'userDisplayName', 'isVerified', 'hasSpoilers', 'userRealm', 'hasProfanity', 'isSuperReviewer', 'rating'], axis=1)\n",
    "\n",
    "    df_cleaned.columns = [camel_to_snake_manual(col) for col in df_cleaned.columns]\n",
    "\n",
    "    def standardize_score(score):\n",
    "        return min(10, max(1, round(score * 2)))\n",
    "\n",
    "    df_cleaned['standardized_score'] = df_cleaned['score'].apply(standardize_score)\n",
    "\n",
    "    df_cleaned['creation_date'] = pd.to_datetime(df_cleaned['creation_date'], errors='coerce')\n",
    "    df_cleaned['creation_year'] = df_cleaned['creation_date'].dt.year\n",
    "    df_cleaned = df_cleaned.drop(columns=['creation_date'])\n",
    "\n",
    "    return df_cleaned\n",
    "\n",
    "sampled_user_reviews_cleaned = clean_user_reviews(user_reviews)\n",
    "print(\"User reviews cleaned.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biPMI9XBCjDC",
   "metadata": {
    "id": "biPMI9XBCjDC"
   },
   "source": [
    "### 3.5. Merging Datasets\n",
    "We merged the three cleaned datasets (movies, critic reviews, and user reviews) using the merge_datasets() function. This function:\n",
    "\n",
    "- Aggregates critic and user reviews to calculate average scores.\n",
    "- Combines quotes from both critics and users.\n",
    "- Keeps existing sentiments from the original movie dataset.\n",
    "\n",
    "This comprehensive dataset provides a complete picture, combining both critic and user perspectives on each movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d9fde5",
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "aborted",
     "timestamp": 1728831674078,
     "user": {
      "displayName": "M.M.",
      "userId": "16283090766959631234"
     },
     "user_tz": -120
    },
    "id": "e1d9fde5"
   },
   "outputs": [],
   "source": [
    "movies_cleaned[[\"movie_title\",\"rating\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baad8b17",
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "aborted",
     "timestamp": 1728831674078,
     "user": {
      "displayName": "M.M.",
      "userId": "16283090766959631234"
     },
     "user_tz": -120
    },
    "id": "baad8b17"
   },
   "outputs": [],
   "source": [
    "# Count the occurrences of \"Unknown\" and \"unknown\" in the 'rating' column of movies_clean\n",
    "rating_counts = movies_cleaned['rating'].value_counts()\n",
    "\n",
    "# Extract counts for \"Unknown\" and \"unknown\"\n",
    "count_unknown = rating_counts.get(\"Unknown\", 0)  # Returns 0 if \"Unknown\" is not found\n",
    "count_unknown_lower = rating_counts.get(\"unknown\", 0)  # Returns 0 if \"unknown\" is not found\n",
    "\n",
    "# Display the results\n",
    "print(f\"Count of 'Unknown': {count_unknown}\")\n",
    "print(f\"Count of 'unknown': {count_unknown_lower}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219186a2",
   "metadata": {
    "id": "219186a2"
   },
   "source": [
    "## 4. Data Analysis of the movies data \n",
    "\n",
    "This section shows the data analysis and the corresponding data visualization for each of the following parts:<br/>\n",
    "\n",
    "- \"Most popular movies of all time\"<br/>\n",
    "- \"Most popular 100 movies by critic score\"<br/>\n",
    "- \"Most popular 100 movies by combined critic & audience score\".<br/>\n",
    "\n",
    "For this part of the project the clean dataframe called \"movies_cleaned\" will be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03463e92",
   "metadata": {
    "id": "03463e92"
   },
   "source": [
    "### 4.1. Most popular 100 movies by audience score\n",
    "To identify the 100 most popular movies of all time, we used audience_score as respresentative of popularity, ranked them and listed the top 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "L2noD1DecHb5",
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "aborted",
     "timestamp": 1728831674078,
     "user": {
      "displayName": "M.M.",
      "userId": "16283090766959631234"
     },
     "user_tz": -120
    },
    "id": "L2noD1DecHb5"
   },
   "outputs": [],
   "source": [
    "# most popular movie based on the audience score\n",
    "\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "top_100_movies = movies_cleaned.sort_values(by=['audience_score', 'movie_year'], ascending=[False, True]).head(100)\n",
    "\n",
    "top_100_movies[['movie_title', 'audience_score', 'movie_year']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf10f685",
   "metadata": {
    "id": "cf10f685"
   },
   "source": [
    "### 4.2. Most popular 100 movies by critic score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12e6f23",
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "aborted",
     "timestamp": 1728831674078,
     "user": {
      "displayName": "M.M.",
      "userId": "16283090766959631234"
     },
     "user_tz": -120
    },
    "id": "c12e6f23"
   },
   "outputs": [],
   "source": [
    "# Sort the movies based on critic score and movie year, and get the top 100\n",
    "top_100_movies = movies_cleaned.sort_values(by=['critic_score', 'movie_year'], ascending=[False, True]).head(100)\n",
    "\n",
    "# Define 10-year bins and create a new column to group movie years by decade\n",
    "bins = range(1900, 2030, 10)  # 10-year bins\n",
    "labels = [f'{b}-{b+9}' for b in bins[:-1]]  # Create labels for the bins\n",
    "\n",
    "# Create a new column 'decade' in top_100_movies to categorize movies into 10-year intervals\n",
    "top_100_movies['decade'] = pd.cut(top_100_movies['movie_year'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# Count the number of movies in each decade\n",
    "decade_counts = top_100_movies['decade'].value_counts().sort_index()\n",
    "\n",
    "# Set global parameters for text anti-aliasing and sharpness\n",
    "plt.rcParams['text.antialiased'] = True\n",
    "plt.rcParams['figure.dpi'] = 150  # Increase DPI for better text sharpness\n",
    "\n",
    "# Set up the figure with high DPI for sharper text\n",
    "plt.figure(figsize=(14, 8))  # Increased figure size for better display\n",
    "\n",
    "# Plot a bar plot with Seaborn for separated bars\n",
    "ax = sns.barplot(x=decade_counts.index, y=decade_counts.values, edgecolor='black', color='#1f77b4')  # Original blue color\n",
    "\n",
    "# Set titles and labels with anti-aliasing enabled by default, add pad to title for spacing\n",
    "plt.title('Distribution of the Most Popular 100 Movies by Critic Score (Grouped by Decade)', fontsize=16, pad=20)  # Adjust the pad value for distance\n",
    "plt.xlabel('Decade', fontsize=12)\n",
    "plt.ylabel('Number of Movies', fontsize=12)\n",
    "\n",
    "# Remove the top spine (frame line above the plot)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "# Add numbers above each bar\n",
    "for index, value in enumerate(decade_counts.values):\n",
    "    plt.text(index, value + 0.5, str(value), ha='center', fontsize=10)\n",
    "\n",
    "# Adjust the layout for better presentation\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qHxwSgVcIKes",
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "aborted",
     "timestamp": 1728831674078,
     "user": {
      "displayName": "M.M.",
      "userId": "16283090766959631234"
     },
     "user_tz": -120
    },
    "id": "qHxwSgVcIKes"
   },
   "outputs": [],
   "source": [
    "# Get the top 100 most popular movies based on critic_score\n",
    "top_100_movies_critic_score = movies_cleaned.nlargest(100, 'critic_score')\n",
    "\n",
    "# Create year bins/intervals for every 10 years (including \"1910-1919\")\n",
    "bins = [1900, 1910, 1920, 1930, 1940, 1950, 1960, 1970, 1980, 1990, 2000, 2010, 2020]\n",
    "labels = ['1900-1909', '1910-1919', '1920-1929', '1930-1939', '1940-1949', '1950-1959',\n",
    "          '1960-1969', '1970-1979', '1980-1989', '1990-1999', '2000-2009', '2010-2019']\n",
    "\n",
    "# Bin the movie years into intervals, even if a bin is empty\n",
    "top_100_movies_critic_score['year_interval'] = pd.cut(top_100_movies_critic_score['movie_year'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# Create a new DataFrame to consolidate movie titles by year interval, sorted by year\n",
    "hover_data = top_100_movies_critic_score.groupby('year_interval', observed=False).apply(\n",
    "    lambda x: '<br>• '.join(sorted([f\"{title} ({year})\" for title, year in zip(x['movie_title'], x['movie_year'])], key=lambda y: int(y.split('(')[-1][:-1])))).reset_index(name='movie_title_list')\n",
    "\n",
    "# Ensure the first movie title in each hover starts with a bullet point\n",
    "hover_data['movie_title_list'] = '• ' + hover_data['movie_title_list']\n",
    "\n",
    "# Merge back to include movie titles in the main DataFrame\n",
    "top_100_movies_critic_score = top_100_movies_critic_score.merge(hover_data, on='year_interval', suffixes=('', '_list'), how='right')\n",
    "\n",
    "# Create the bar plot\n",
    "fig = px.bar(\n",
    "    top_100_movies_critic_score,\n",
    "    x='year_interval',\n",
    "    y='critic_score',  # This is just a dummy to create the bars, it doesn't matter\n",
    "    color_discrete_sequence=['green'],\n",
    "    hover_name='year_interval',\n",
    "    hover_data={'movie_title_list': True},  # Show the list of movie titles on hover\n",
    "    title='Top 100 Movies by Critic Score (Interactive Graph)',\n",
    "    labels={'year_interval': 'Year Interval', 'critic_score': 'Movies Count'}\n",
    ")\n",
    "\n",
    "# Customize hover to show movie titles and other desired info\n",
    "fig.update_traces(hovertemplate='Movies with best critic score (from past to present):<br><br>%{customdata[0]}')\n",
    "\n",
    "# Remove y-axis labels since they aren't meaningful here\n",
    "fig.update_layout(yaxis_title='', yaxis=dict(showticklabels=False))\n",
    "\n",
    "# Center the title and add the message below it with a small distance from the graph\n",
    "fig.update_layout(\n",
    "    title_x=0.5,  # Center the title\n",
    "    annotations=[\n",
    "        dict(\n",
    "            text=\"All these movies have been rated 100 by the critic. The graph shows these movies grouped by decade.<br>Go with the mouse on a bar to see the list of movies.\",\n",
    "            xref=\"paper\", yref=\"paper\",\n",
    "            x=0.5, y=1.1,  # Slightly increased 'y' to add space between text and graph\n",
    "            showarrow=False,\n",
    "            font=dict(size=12),  # Increased font size\n",
    "            align=\"center\"\n",
    "        )\n",
    "    ],\n",
    "    xaxis=dict(tickfont=dict(size=10))  # Adjust the font size of x-axis labels (decades)\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a345fc",
   "metadata": {
    "id": "70a345fc"
   },
   "source": [
    "### 4.3. Top 100 movies by \"combined critic & audience score\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a40db7",
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "aborted",
     "timestamp": 1728831674078,
     "user": {
      "displayName": "M.M.",
      "userId": "16283090766959631234"
     },
     "user_tz": -120
    },
    "id": "42a40db7"
   },
   "outputs": [],
   "source": [
    "# Calculate the combined score as the average of critic_score and audience_score\n",
    "movies_cleaned['combined_score'] = (movies_cleaned['critic_score'] + movies_cleaned['audience_score']) / 2\n",
    "\n",
    "# Get the top 100 movies based on the combined score\n",
    "top_100_combined_score = movies_cleaned.nlargest(100, 'combined_score')\n",
    "\n",
    "# Create a new DataFrame to consolidate movie titles by combined score\n",
    "hover_data = top_100_combined_score.groupby('combined_score').apply(\n",
    "    lambda x: '<br>• '.join(sorted([f\"{title} ({year})\" for title, year in zip(x['movie_title'], x['movie_year'])], key=lambda y: int(y.split('(')[-1][:-1])))\n",
    ").reset_index(name='movie_title_list')\n",
    "\n",
    "# Merge back to include movie titles in the main DataFrame\n",
    "top_100_combined_score = top_100_combined_score.merge(hover_data, on='combined_score')\n",
    "\n",
    "# Create the scatter plot\n",
    "fig = px.scatter(\n",
    "    top_100_combined_score,\n",
    "    x='combined_score',\n",
    "    y='combined_score',  # Dummy y-value for the dots\n",
    "    hover_name='combined_score',\n",
    "    hover_data={'movie_title_list': True},  # Show the list of movie titles\n",
    "    title='Top 100 Movies by Combined Critic and Audience Score (interactive graph)',\n",
    "    size='combined_score',  # Size based on the combined score\n",
    "    size_max=20,  # Adjust the size of the dots\n",
    ")\n",
    "\n",
    "# Update hover template to show bullet points with a line break\n",
    "fig.update_traces(hovertemplate='Movie titles (from past to present):<br><br>• %{customdata[0]}')\n",
    "\n",
    "# Center the title\n",
    "fig.update_layout(title_x=0.5)\n",
    "\n",
    "# Remove Y-axis as it is not meaningful\n",
    "fig.update_layout(yaxis_title='', yaxis=dict(showticklabels=False))\n",
    "\n",
    "# Show the plot\n",
    "fig.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b18fc5a",
   "metadata": {
    "id": "4b18fc5a"
   },
   "source": [
    "## 5. Data Science Methods\n",
    "In this chapter, we delve into the application of two widely recognized and powerful data science methodologies: Multiple Linear Regression and Natural Language Processing (NLP). These techniques have been instrumental in extracting deeper insights from the datasets of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357efa76",
   "metadata": {
    "id": "357efa76"
   },
   "source": [
    "### 5.1. Multiple Linear Regression\n",
    "Multiple Linear Regression is a statistical approach used to model the relationship between multiple independent variables and a dependent variable. This method enables the identification and quantification of the influence of various factors on a particular outcome, thereby providing valuable insights into the underlying patterns within the data. By employing Multiple Linear Regression, significant relationships can be uncovered that enhance the understanding of trends and predictions in the movie industry.\n",
    "<br/>\n",
    "<br/>\n",
    "For the development of the linear regression model, only variables from the \"movies\" dataset were utilized. To facilitate this process, a copy of the \"movies_cleaned\" DataFrame was created, ensuring that the original data remained intact while allowing for the necessary analyses to be conducted. Variables from the \"critic reviews\" and \"user reviews\" datasets were excluded from this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a0c129",
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "aborted",
     "timestamp": 1728831674078,
     "user": {
      "displayName": "M.M.",
      "userId": "16283090766959631234"
     },
     "user_tz": -120
    },
    "id": "86a0c129"
   },
   "outputs": [],
   "source": [
    "# create a copy of the movies_cleaned dataset\n",
    "movies_cleaned2 = movies_cleaned.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1d149d",
   "metadata": {
    "id": "1d1d149d"
   },
   "source": [
    "For our model we will use the following variables:\n",
    "- \"audience_score\" as dependent variable;\n",
    "- \"rating\",\"runtime_in_minutes\" and \"original_language\" as independent variables.\n",
    "\n",
    "Before building the linear regression model we will make some adjustments:\n",
    "\n",
    "- we will work on the variable \"rating\";\n",
    "- we will do some one-hot encoding;\n",
    "- we will drop the rows with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22edb3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_cleaned2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93156831",
   "metadata": {},
   "source": [
    "#### 5.1.1. Variable \"rating\"\n",
    "\n",
    "The variable \"rating\" has many different values (see below): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a943e152-317e-4a21-933b-7ea94215675a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#movies_cleaned2.unique()\n",
    "list(movies_cleaned2.rating.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4e8a30-c7a0-4be3-9417-3cfca73f8e88",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "These values can be grouped in bigger categories. In other words for our regression model we chose to not investigate the rating into detail and therefore we will ignore the details in the braquets and simply consider the general rating of the movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009892ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_cleaned2['rating'] = movies_cleaned2['rating'].str.extract(r'([A-Z]+(?:-[0-9]+)?)')\n",
    "\n",
    "# remove \"nan\" values in the column \"rating\"\n",
    "movies_cleaned2 = movies_cleaned2[movies_cleaned2['rating'].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18cb00c",
   "metadata": {},
   "source": [
    "New values after grouping into the main ratings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2468d380",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(movies_cleaned2.rating.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6922a4",
   "metadata": {},
   "source": [
    "#### 5.1.2. Variable \"runtime_in_minutes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3551ba67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.scatterplot(x=movies_cleaned2['runtime_in_minutes'], y=movies_cleaned2['audience_score'])\n",
    "plt.xlabel('Runtime in Minutes')\n",
    "plt.ylabel('Audience Score')\n",
    "plt.title('Runtime vs Audience Score')\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4519f0d-5ce4-4d1b-872f-a1b0c498e238",
   "metadata": {},
   "source": [
    "Based on the scatterplot, there doesn't appear to be a strong linear correlation between \"Runtime in Minutes\" and \"Audience Score.\" Including \"Runtime in Minutes\" in our linear regression model might not significantly improve its predictive power. This is confirmed by the following correlation coefficient (see below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd2f798-78e2-4519-a086-a3d358f64915",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = movies_cleaned2['runtime_in_minutes'].corr(movies_cleaned2['audience_score'])\n",
    "print(correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78718ae0-2587-4f42-a92b-23b9321560ee",
   "metadata": {},
   "source": [
    "#### 5.1.3. Variable \"critic_score\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305dfc20-4894-4e00-858a-5e6acbd05f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.scatterplot(x=movies_cleaned2['critic_score'], y=movies_cleaned2['audience_score'])\n",
    "plt.xlabel('Critic Score')\n",
    "plt.ylabel('Audience Score')\n",
    "plt.title('Critic Score vs Audience Score')\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04abc2bb-1e0d-4d8b-a9c9-c2133927bc95",
   "metadata": {},
   "source": [
    "Based on the scatterplot, there seems to be to be a linear correlation between \"Critic Score\" and \"Audience Score\" that is worth bein investigated further. This is confirmed by the following correlation coefficient (see below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45171af0-a8ef-4943-bf9b-72b337897af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = movies_cleaned2['critic_score'].corr(movies_cleaned2['audience_score'])\n",
    "print(correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246a36ff-8fba-4e62-8271-aed0e8cb166c",
   "metadata": {},
   "source": [
    "Therefore we will keep critic score for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd55643-ab12-4007-a68b-849d1152be69",
   "metadata": {},
   "source": [
    "#### 5.1.4. Dropping irrelevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e9f4a3-0963-404a-8d50-cf07d75d5a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_cleaned2 = movies_cleaned2.drop(columns=['movie_year','movie_id', 'movie_title', 'original_language', 'release_year_theaters', 'release_year_streaming', \n",
    "                                               'runtime_in_minutes', 'combined_score'])\n",
    "\n",
    "movies_cleaned2.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035cea2d",
   "metadata": {},
   "source": [
    "#### 5.1.5. One-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fde149-063a-4f69-855f-792a0a0a76b2",
   "metadata": {},
   "source": [
    "We apply \"One-hot encoding\" for the following non-numerical variables: 'critic_sentiment','audience_sentiment', 'rating'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ea75fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Encode the categorical variables 'critic_sentiment' and 'rating'\n",
    "movies_cleaned2_encoded = pd.get_dummies(movies_cleaned2, columns=['critic_sentiment','audience_sentiment', 'rating'], drop_first=True)\n",
    "\n",
    "# Columns after encoding will look like:\n",
    "# 'critic_sentiment_positive', 'rating_R', 'rating_PG-13', etc.\n",
    "movies_cleaned2_encoded.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cab3626-5d1f-4aac-b99a-00d99ff76c9e",
   "metadata": {},
   "source": [
    "#### 5.1.6. Conversion of encoded variables from boolean into dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75a2828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine ratings into broader categories\n",
    "movies_cleaned2_encoded['rating_Teen'] = (\n",
    "    movies_cleaned2_encoded['rating_PG-13'] + movies_cleaned2_encoded['rating_TV-14']\n",
    ")\n",
    "\n",
    "movies_cleaned2_encoded['rating_Mature'] = (\n",
    "    movies_cleaned2_encoded['rating_NC-17'] + movies_cleaned2_encoded['rating_R']\n",
    ")\n",
    "\n",
    "# Drop the original columns\n",
    "#movies_cleaned2_encoded.drop(\n",
    "#    columns=['rating_NC-17', 'rating_PG-13', 'rating_R', 'rating_TV', 'rating_TV-14'],\n",
    "#    inplace=True\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72803255-6e07-48a6-b991-8451474ea87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique values for each variable\n",
    "print(\"Unique values in 'critic_score':\", movies_cleaned2_encoded['critic_score'].unique())\n",
    "print(\"Unique values in 'critic_sentiment_positive':\", movies_cleaned2_encoded['critic_sentiment_positive'].unique())\n",
    "print(\"Unique values in 'audience_sentiment_positive':\", movies_cleaned2_encoded['audience_sentiment_positive'].unique())\n",
    "print(\"Unique values in 'rating_PG':\", movies_cleaned2_encoded['rating_PG'].unique())\n",
    "print(\"Unique values in 'rating_Teen':\", movies_cleaned2_encoded['rating_Teen'].unique())\n",
    "print(\"Unique values in 'rating_Mature':\", movies_cleaned2_encoded['rating_Mature'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9ba5c6-4055-4463-b620-b1feecabac16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert boolean columns to integers (True -> 1, False -> 0)\n",
    "movies_cleaned2_encoded['critic_sentiment_positive'] = movies_cleaned2_encoded['critic_sentiment_positive'].astype(int)\n",
    "movies_cleaned2_encoded['critic_sentiment_positive'] = 1 - movies_cleaned2_encoded['critic_sentiment_positive']\n",
    "movies_cleaned2_encoded['audience_sentiment_positive'] = movies_cleaned2_encoded['audience_sentiment_positive'].astype(int)\n",
    "movies_cleaned2_encoded['audience_sentiment_positive'] = 1 - movies_cleaned2_encoded['audience_sentiment_positive']\n",
    "movies_cleaned2_encoded['rating_PG'] = movies_cleaned2_encoded['rating_PG'].astype(int)\n",
    "movies_cleaned2_encoded['rating_Teen'] = movies_cleaned2_encoded['rating_Teen'].astype(int)\n",
    "movies_cleaned2_encoded['rating_Mature'] = movies_cleaned2_encoded['rating_Mature'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a0064d-c04e-415c-9e92-ff0ec29d3489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the unique values again\n",
    "print(f\"Unique values in 'critic_sentiment_positive': {movies_cleaned2_encoded['critic_sentiment_positive'].unique()}\")\n",
    "print(f\"Unique values in 'audience_sentiment_positive': {movies_cleaned2_encoded['audience_sentiment_positive'].unique()}\")\n",
    "print(f\"Unique values in 'rating_PG': {movies_cleaned2_encoded['rating_PG'].unique()}\")\n",
    "print(f\"Unique values in 'rating_Teen': {movies_cleaned2_encoded['rating_Teen'].unique()}\")\n",
    "print(f\"Unique values in 'rating_Mature': {movies_cleaned2_encoded['rating_Mature'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65eaf0db",
   "metadata": {},
   "source": [
    "#### 5.1.7. Handling missing values"
   ]
  },
  {
   "cell_type": "raw",
   "id": "847c0230-cf4c-44cc-8ca9-cac2ae736749",
   "metadata": {},
   "source": [
    "# Check for missing values in the relevant columns\n",
    "print(movies_cleaned2_encoded[['audience_score','critic_score', 'runtime_in_minutes'] + \n",
    "               [col for col in movies_cleaned2_encoded.columns if 'audience_sentiment' in col or 'critic_sentiment' in col or 'original_language_' in col or 'rating_' in col]].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57639dee-235b-4109-9cc7-986c51fc2237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the relevant columns\n",
    "print(movies_cleaned2_encoded[['audience_score','critic_score'] + \n",
    "               [col for col in movies_cleaned2_encoded.columns if 'audience_sentiment' in col or 'critic_sentiment' in col or 'rating_' in col]].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd767ee7-3d11-44b8-aa53-8b3be55e0318",
   "metadata": {},
   "source": [
    "#### 5.1.8. Drop the columns with \"unknown\" values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb04cff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_cleaned2_encoded = movies_cleaned2_encoded.drop(columns=[\"critic_sentiment_unknown\", \"audience_sentiment_unknown\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1883981",
   "metadata": {},
   "source": [
    "#### 5.1.9. Multicollinearity: calculation of the Variance Inflation Factor (VIF)\n",
    "\n",
    "Before building the model, we want to check if there is multicollinearity. Multicollinearity occurs when two or more independent variables are highly correlated, which can make it difficult for the model to determine their individual effects on the dependent variable.\n",
    "\n",
    "Variance Inflation Factor measures multicollinearity among the independent variables themselves. So, a high VIF indicates that a predictor has a high linear relationship with one or more of the other predictors in the model, not with the dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8e086b-4c32-4c07-9f53-8d1f8ae2c81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "import pandas as pd\n",
    "\n",
    "# Define the independent variables\n",
    "X = movies_cleaned2_encoded[['critic_score', 'audience_sentiment_positive', 'critic_sentiment_positive', 'rating_PG', 'rating_Teen', 'rating_Mature']]\n",
    "\n",
    "# Add a constant (intercept) to the model\n",
    "X = add_constant(X)\n",
    "\n",
    "# Calculate VIF for each variable\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Variable\"] = X.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99349a13",
   "metadata": {},
   "source": [
    "Variables with VIF > 5:\n",
    "\n",
    "- rating_Teen (5.53)\n",
    "- rating_Mature (6.28)\n",
    "\n",
    "These two rating variables exhibit moderate to high multicollinearity, which can potentially distort the regression coefficients and make it difficult to assess the individual effect of each variable. \n",
    "\n",
    "We will build the model first including both variables and then excluding one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4718ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "import pandas as pd\n",
    "\n",
    "# Add a constant (intercept) to the independent variables\n",
    "x_with_constant = add_constant(movies_cleaned2_encoded[['audience_score', 'audience_sentiment_positive']])\n",
    "\n",
    "# Calculate VIF\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Variable\"] = x_with_constant.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(x_with_constant.values, i) for i in range(x_with_constant.shape[1])]\n",
    "\n",
    "# Print the VIF values\n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80af52c1",
   "metadata": {},
   "source": [
    "#### 5.1.10. Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a99eb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the independent variables (features) and the dependent variable\n",
    "x = movies_cleaned2_encoded[['critic_score','critic_sentiment_positive', \n",
    "                             'audience_sentiment_positive',\n",
    "                             #'rating_PG', 'rating_Teen']]                  # 0.772\n",
    "                             #'rating_PG', 'rating_Mature']]                # 0.771\n",
    "                             'rating_PG', 'rating_Teen', 'rating_Mature']]  # 0.772\n",
    "                          \n",
    "y = movies_cleaned2_encoded['audience_score']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state=42)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(x_train, y_train)\n",
    "pd.DataFrame(model.coef_, x.columns, columns = ['Coeff'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc9ddfc",
   "metadata": {},
   "source": [
    "#### 5.1.11. Interpretation of the results\n",
    "\n",
    "- critic_score (0.2): the coefficient is lower than expected, yet it is a positive correlation as expected positive. Movies with positive critic sentiment are predicted to have an audience score 0.2 points higher, on average, compared to those without positive critic score, holding other variables constant.\n",
    "<br>\n",
    "\n",
    "- critic_sentiment_positive (1): Movies with positive critic sentiment are predicted to have an audience score 0.9 points higher, on average, compared to those without positive critic sentiment, holding other variables constant.\n",
    "<br>\n",
    "\n",
    "- audience_sentiment_positive (29): A positive audience sentiment is associated with a significant 29.2-point increase in the predicted audience score, on average, assuming all else remains constant. This makes sense as audience sentiment is closely related to the audience score.\n",
    "<br>\n",
    "- rating_PG (2.5): Movies with a PG rating are predicted to have an audience score 2.5 points higher than those without this rating, assuming other variables are constant.\n",
    "<br>\n",
    "- rating_Teen (2.1): Movies targeting a \"Teen\" audience are predicted to have a slightly higher audience score by 2.1 points, on average, compared to those not targeting teens. \n",
    "<br>\n",
    "- rating_Mature (-0.3): Movies rated for a \"Mature\" audience are predicted to have an audience score 0.3 points lower, on average, compared to those without this rating, assuming other variables remain constant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2351ee7",
   "metadata": {},
   "source": [
    "#### 5.1.12. Accuracy of the Linear Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2be4f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "# Calculate R-squared\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(f\"R-squared: {r2}. This means that 77.2% of the variance in audience_score is explained by the independent variables in your model. This is a strong value, suggesting your model provides a good fit to the data.\")\n",
    "print(\"\")\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(\"\")\n",
    "# Calculate variance and standard deviation of audience_score\n",
    "variance = y_train.var()\n",
    "std_dev = y_train.std()\n",
    "\n",
    "print(\"Variance:\", variance)\n",
    "print(\"\")\n",
    "print(\"Standard Deviation:\", std_dev)\n",
    "print(\"\")\n",
    "if mse < variance:\n",
    "    print(\"Mean Squared Error < Variance, hence the model captures much of the variability in audience_score and is performing well.\")\n",
    "else:\n",
    "    print(\"The model predictions are not much better than just using the average value as the prediction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2408cd7",
   "metadata": {},
   "source": [
    "**End of the Linear Regression Model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ba112b",
   "metadata": {
    "id": "d7ba112b"
   },
   "source": [
    "### 5.2. Natural Language Processing\n",
    "Natural Language Processing (NLP) is a branch of artificial intelligence that enables computers to understand, interpret, and generate human language. Through the implementation of NLP techniques, we can analyze textual data—such as movie reviews—transforming unstructured information into actionable insights. This allows us to assess sentiment, extract themes, and gauge audience reactions, enhancing our overall understanding of viewer perceptions and preferences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dOl4EsyS-twu",
   "metadata": {
    "id": "dOl4EsyS-twu"
   },
   "source": [
    "#### 5.2.1. Cleaning Quotes and Filtering Nouns and Verbs\n",
    "The first step in our analysis involved cleaning the movie review quotes. We used the process_quotes() function to remove stop words, which are common but uninformative words such as \"the,\" \"is,\" and \"and.\" Additionally, we defined a list of words to exclude, such as general movie-related terms (e.g., \"movie,\" \"film\") and quality descriptors (e.g., \"good,\" \"bad\"). By removing these words, we focused the analysis on those words that provide deeper insight into the content and themes of the reviews.\n",
    "\n",
    "After cleaning, we filtered each cleaned quote to retain only nouns and verbs. This was done using part-of-speech tagging to identify words representing core ideas or actions. This allowed us to focus specifically on key themes, such as \"love,\" \"war,\" or \"acting,\" and highlight the main concepts expressed in the reviews while eliminating less important parts of speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed53f95-a723-4013-95b3-7287897c675a",
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "aborted",
     "timestamp": 1728831674079,
     "user": {
      "displayName": "M.M.",
      "userId": "16283090766959631234"
     },
     "user_tz": -120
    },
    "id": "5ed53f95-a723-4013-95b3-7287897c675a"
   },
   "outputs": [],
   "source": [
    "# Download necessary NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# Define stop words and words to exclude\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words_to_exclude = {\n",
    "    # General movie-related terms\n",
    "    'movie', 'film', 'cinema', 'picture', 'flick', 'screening', 'showing',\n",
    "\n",
    "    # Quality descriptors\n",
    "    'good', 'bad', 'great', 'terrible', 'excellent', 'poor', 'amazing', 'awful',\n",
    "    'wonderful', 'horrible', 'best', 'worst', 'favorite', 'liked', 'disliked',\n",
    "\n",
    "    # Cinema-related terms\n",
    "    'actor', 'actress', 'director', 'producer', 'screenplay', 'script', 'scene',\n",
    "    'character', 'plot', 'story', 'dialogue', 'cinematography', 'soundtrack',\n",
    "\n",
    "    # General descriptors\n",
    "    'interesting', 'boring', 'exciting', 'dull', 'entertaining', 'disappointing',\n",
    "    'impressive', 'mediocre', 'overrated', 'underrated',\n",
    "\n",
    "    # Time-related terms\n",
    "    'hour', 'minute', 'long', 'short',\n",
    "\n",
    "    # Viewing experience\n",
    "    'watch', 'saw', 'seen', 'theater', 'cinema', 'home',\n",
    "\n",
    "    # Rating-related terms\n",
    "    'star', 'rating', 'review', 'critic', 'audience',\n",
    "\n",
    "    # Production-related terms\n",
    "    'budget', 'box office', 'sequel', 'remake', 'adaptation',\n",
    "\n",
    "    # General opinion words\n",
    "    'think', 'thought', 'feel', 'felt', 'believe', 'opinion'\n",
    "}\n",
    "stop_words.update(words_to_exclude)\n",
    "\n",
    "def process_quote(quote):\n",
    "    # Convert to string if it's a list\n",
    "    if isinstance(quote, list):\n",
    "        quote = ' '.join(quote)\n",
    "    else:\n",
    "        quote = str(quote)\n",
    "\n",
    "    # Tokenize and remove stop words and excluded words\n",
    "    tokens = word_tokenize(quote.lower())\n",
    "    tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
    "\n",
    "    # Perform POS tagging\n",
    "    tagged = pos_tag(tokens)\n",
    "\n",
    "    # Filter for nouns and verbs\n",
    "    filtered = [word for word, pos in tagged if pos.startswith('N') or pos.startswith('V')]\n",
    "\n",
    "    return ' '.join(filtered)\n",
    "\n",
    "def process_quotes_in_chunks(df, chunk_size=1000):\n",
    "    processed_chunks = []\n",
    "    total_chunks = len(df) // chunk_size + (1 if len(df) % chunk_size != 0 else 0)\n",
    "\n",
    "    for i in tqdm(range(0, len(df), chunk_size), total=total_chunks, desc=\"Processing chunks\"):\n",
    "        try:\n",
    "            chunk = df.iloc[i:i+chunk_size].copy()\n",
    "            chunk['filtered_quotes'] = chunk['all_quotes'].apply(process_quote)\n",
    "            processed_chunks.append(chunk)\n",
    "            print(f\"Processed chunk {i//chunk_size + 1}/{total_chunks}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing chunk {i//chunk_size + 1}: {str(e)}\")\n",
    "\n",
    "    return pd.concat(processed_chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abZfisiHibUx",
   "metadata": {
    "id": "abZfisiHibUx"
   },
   "source": [
    "#### 5.2.2. Word Cloud of Themes\n",
    "\n",
    "To visualize the most common themes discussed in the movie reviews, we generated a word cloud. The word cloud was created using the cleaned dataset to present an intuitive overview of frequently mentioned topics. By emphasizing the prominent words, it gives a clear picture of what audiences and critics frequently focus on when evaluating movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2f1b2f-178b-4f8b-a7ef-ad6060ef1506",
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "aborted",
     "timestamp": 1728831674080,
     "user": {
      "displayName": "M.M.",
      "userId": "16283090766959631234"
     },
     "user_tz": -120
    },
    "id": "ee2f1b2f-178b-4f8b-a7ef-ad6060ef1506"
   },
   "outputs": [],
   "source": [
    "# Load the processed data\n",
    "\n",
    "import gdown\n",
    "import pandas as pd\n",
    "\n",
    "# Google Drive file ID for the \"processed_movie_quotes.csv\" file\n",
    "file_id = '1pPXkjjZaYkFFLShvgCUG53yILh9_SW04'\n",
    "\n",
    "# Import the \"processed_movie_quotes.csv\" file using gdown\n",
    "gdown.download(f'https://drive.google.com/uc?export=download&id={file_id}', 'processed_movie_quotes.csv', quiet=True)\n",
    "\n",
    "# Load the downloaded \"processed_movie_quotes.csv\" file into a pandas DataFrame\n",
    "df = pd.read_csv('processed_movie_quotes.csv')\n",
    "\n",
    "# Replace NaN with an empty string and convert all values to strings\n",
    "df['filtered_quotes'] = df['filtered_quotes'].fillna('').astype(str)\n",
    "\n",
    "# Remove any empty strings\n",
    "df = df[df['filtered_quotes'] != '']\n",
    "\n",
    "print(\"Number of rows after cleaning:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a98be7a-1719-4681-845e-a6a72c2bc1ca",
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "aborted",
     "timestamp": 1728831674080,
     "user": {
      "displayName": "M.M.",
      "userId": "16283090766959631234"
     },
     "user_tz": -120
    },
    "id": "9a98be7a-1719-4681-845e-a6a72c2bc1ca"
   },
   "outputs": [],
   "source": [
    "# Function to process quotes in chunks\n",
    "def process_quotes_chunk(chunk):\n",
    "    return Counter(' '.join(chunk).split())\n",
    "\n",
    "# Load and process data in chunks\n",
    "chunk_size = 1000  # Adjust based on your system's capacity\n",
    "word_freq = Counter()\n",
    "\n",
    "print(\"Processing quotes in chunks...\")\n",
    "start_time = time.time()\n",
    "\n",
    "for chunk in (df[i:i + chunk_size] for i in range(0, len(df), chunk_size)):\n",
    "    chunk['filtered_quotes'] = chunk['filtered_quotes'].fillna('').astype(str)\n",
    "    chunk = chunk[chunk['filtered_quotes'] != '']\n",
    "    word_freq.update(process_quotes_chunk(chunk['filtered_quotes']))\n",
    "\n",
    "    # Clear some memory\n",
    "    del chunk\n",
    "    gc.collect()\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Time taken to process all chunks: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Keep only words that appear more than a certain number of times\n",
    "min_freq = 10\n",
    "frequent_words = {word: count for word, count in word_freq.items() if count > min_freq}\n",
    "\n",
    "print(f\"Total unique words: {len(word_freq)}\")\n",
    "print(f\"Words appearing more than {min_freq} times: {len(frequent_words)}\")\n",
    "\n",
    "# Create a word cloud\n",
    "def create_word_cloud(word_freq):\n",
    "    wordcloud = WordCloud(width=1600, height=800,\n",
    "                          background_color='white',\n",
    "                          max_words=500,\n",
    "                          relative_scaling=0.5).generate_from_frequencies(word_freq)\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title('Word Cloud of Movie Quotes (Full Dataset)')\n",
    "    plt.show();\n",
    "\n",
    "# Generate word cloud\n",
    "print(\"Generating word cloud...\")\n",
    "start_time = time.time()\n",
    "create_word_cloud(frequent_words)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time taken to generate word cloud: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e3a36a-5862-4ccf-9794-cd4ef440c6e2",
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "aborted",
     "timestamp": 1728831674080,
     "user": {
      "displayName": "M.M.",
      "userId": "16283090766959631234"
     },
     "user_tz": -120
    },
    "id": "b6e3a36a-5862-4ccf-9794-cd4ef440c6e2"
   },
   "outputs": [],
   "source": [
    "# Separate good and bad movies\n",
    "median_score = df['average_combined_score'].median()\n",
    "good_movies = df[df['average_combined_score'] > median_score]\n",
    "bad_movies = df[df['average_combined_score'] <= median_score]\n",
    "\n",
    "# Create bigram vectorizer\n",
    "vectorizer = CountVectorizer(ngram_range=(2,2), stop_words='english')\n",
    "\n",
    "# Get bigrams for good and bad movies\n",
    "good_bigrams = vectorizer.fit_transform(good_movies['filtered_quotes'])\n",
    "bad_bigrams = vectorizer.transform(bad_movies['filtered_quotes'])\n",
    "\n",
    "# Calculate ratio of frequencies\n",
    "good_freq = good_bigrams.sum(axis=0).A1\n",
    "bad_freq = bad_bigrams.sum(axis=0).A1\n",
    "ratio = np.log((good_freq + 1) / (bad_freq + 1))\n",
    "\n",
    "# Get feature names\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Sort by ratio and get top distinctive bigrams\n",
    "top_good = sorted(zip(feature_names, ratio), key=lambda x: -x[1])[:20]\n",
    "top_bad = sorted(zip(feature_names, ratio), key=lambda x: x[1])[:20]\n",
    "\n",
    "print(\"Bigrams more associated with good movies:\")\n",
    "print(', '.join([f'\"{bg}\"' for bg, _ in top_good]))\n",
    "\n",
    "print(\"\\nBigrams more associated with bad movies:\")\n",
    "print(', '.join([f'\"{bg}\"' for bg, _ in top_bad]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08cddce-c205-4761-a65e-cd554412ccaa",
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "aborted",
     "timestamp": 1728831674080,
     "user": {
      "displayName": "M.M.",
      "userId": "16283090766959631234"
     },
     "user_tz": -120
    },
    "id": "c08cddce-c205-4761-a65e-cd554412ccaa"
   },
   "outputs": [],
   "source": [
    "# Your top 20 word lists\n",
    "highly_rated_top_20 = [\n",
    "    'miyazaki', 'hepburn', 'pacino', 'hitchcock', 'kubrick', 'kurosawa',\n",
    "    'leone', 'bogart', 'chaplin', 'brando', 'goodfellas', 'godfather',\n",
    "    'wilder', 'stewart', 'welles', 'bergman', 'anime', 'coen',\n",
    "    'almodovar', 'newman'\n",
    "]\n",
    "\n",
    "poorly_rated_top_20 = [\n",
    "    'bond', 'sandler', 'cage', 'cruise', 'twists', 'gore', 'blood',\n",
    "    'chemistry', 'zombie', 'williams', 'killer', 'romance', 'cgi',\n",
    "    'vampire', 'carrey', 'adventure', 'murphy', 'surprised',\n",
    "    'sequels', 'suspense', 'confusing'\n",
    "]\n",
    "\n",
    "# Create Counter objects for the word cloud\n",
    "highly_rated_counter = Counter({word: 1 for word in highly_rated_top_20})\n",
    "poorly_rated_counter = Counter({word: 1 for word in poorly_rated_top_20})\n",
    "\n",
    "# Generate word clouds\n",
    "create_word_cloud(highly_rated_counter, 'Word Cloud of Distinctive Words in Highly-Rated Movies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fJsqbN15oHbp",
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "aborted",
     "timestamp": 1728831674080,
     "user": {
      "displayName": "M.M.",
      "userId": "16283090766959631234"
     },
     "user_tz": -120
    },
    "id": "fJsqbN15oHbp"
   },
   "outputs": [],
   "source": [
    "create_word_cloud(poorly_rated_counter, 'Word Cloud of Distinctive Words in Poorly-Rated Movies')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tohlywzmoLlb",
   "metadata": {
    "id": "tohlywzmoLlb"
   },
   "source": [
    "#### 5.2.3. NLP Results\n",
    "\n",
    "The most distinctive words in highly-rated movies included notable directors like \"Miyazaki\" and \"Hitchcock\" as well as iconic actors like \"Pacino\" and \"Hepburn.\" These names suggest that well-known talents significantly contribute to a movie's success. On the other hand, poorly-rated movies had themes like \"gore,\" \"zombie,\" and \"sequels,\" indicating that certain genres or types of movies tend to receive more critical reviews.\n",
    "\n",
    "Insights:\n",
    "- Highly-Rated Movies: The distinctive words associated with highly-rated movies included notable names such as \"Miyazaki,\" \"Hitchcock,\" \"Kubrick,\" and \"Bergman.\" This suggests that acclaimed directors and actors are frequently linked to movies that perform well. Themes like \"anime\" and \"coen\" indicate that certain genres or production styles resonate positively with audiences.\n",
    "\n",
    "- Poorly-Rated Movies: The most distinctive words for poorly-rated movies included \"sandler,\" \"cage,\" \"gore,\" and \"zombie.\" These words suggest that movies featuring certain actors or focusing on specific genres, such as horror, are more likely to receive critical reviews. Words like \"sequels\" and \"twists\" also imply that poorly-executed plot twists or over-reliance on sequels may contribute to lower ratings.\n",
    "\n",
    "These insights help us understand the elements that audiences and critics find appealing or off-putting, providing valuable guidance for movie creators looking to enhance their content and reach wider acclaim."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b8cd84",
   "metadata": {
    "id": "51b8cd84"
   },
   "source": [
    "## 6. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WSQauaWKOy6v",
   "metadata": {
    "id": "WSQauaWKOy6v"
   },
   "source": [
    "This project has been an insightful exploration into the world of data, with significant tasks undertaken and key topics examined in depth. The work demonstrates advanced techniques in data cleaning, manipulation, and visualization. Functions were employed to optimize the code, reducing redundancy and improving overall readability. The data visualization component provided clear and immediate insights into the most popular movies in the industry, utilizing interactive graphs to enhance the presentation and efficiently manage space.\n",
    "\n",
    "The \"Data Science\" section highlighted the power of analytical methods in uncovering patterns that are applicable to real-world scenarios.\n",
    "\n",
    "Furthermore, merging multiple datasets into a cohesive whole was a key element of the project, showcasing an important skill in data analysis. This merged dataset was effectively used in the NLP section to analyze both critic and user reviews, delivering valuable insights into sentiment and opinion within the movie industry."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
